{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Brain Tumor MRI Image Classification\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    -Classification (with elements of EDA and CNN-based Deep Learning)\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focuses on building a deep learning-based classification model to detect and identify brain tumors from MRI images. The four categories considered in this study are: Glioma Tumor, Meningioma Tumor, Pituitary Tumor, and No Tumor. The dataset was divided into training, validation, and testing sets, with all images preprocessed (resized to 128x128, normalized) to ensure uniformity.\n",
        "\n",
        "The project starts with Exploratory Data Analysis (EDA) to understand the image distribution, class balance, and sample quality. Then a Custom Convolutional Neural Network (CNN) was built and trained with different callbacks (EarlyStopping, ReduceLROnPlateau, ModelCheckpoint) to optimize performance. Transfer learning models like VGG16, ResNet50, and EfficientNet were also used and compared to check improvements over the custom CNN.\n",
        "\n",
        "The evaluation metrics used include Accuracy, Precision, Recall, F1-Score, and Confusion Matrix. Additionally, training history was visualized to better understand the learning dynamics over epochs.\n",
        "\n",
        "Finally, the best-performing model was saved and tested on unseen MRI images to check its real-world inference capability. This approach can help radiologists as a second-opinion tool in early and accurate tumor detection, reducing diagnostic time and increasing accuracy."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Rounak36/Brain-Tumor-MRI-Image-Classification"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Brain tumors can be life-threatening if not diagnosed at an early stage. Manual diagnosis through MRI scans is time-consuming and requires expert radiologists, which may not always be accessible, especially in remote areas. This project aims to automate brain tumor classification using deep learning models trained on MRI scans, making the diagnosis faster, more consistent, and scalable. The ultimate goal is to assist medical professionals with reliable AI-driven tools for tumor detection and classification."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.utils.class_weight import compute_class_weight\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "train_df = pd.read_csv('/content/_classes (1).csv')\n",
        "valid_df = pd.read_csv('/content/_classes (2).csv')\n",
        "test_df = pd.read_csv('/content/_classes.csv')\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/content/drive/MyDrive/Tumour/train\"\n",
        "valid_path = \"/content/drive/MyDrive/Tumour/valid\"\n",
        "test_path  = \"/content/drive/MyDrive/Tumour/test\""
      ],
      "metadata": {
        "id": "Tuoqea_PISkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "print(\"Train Set:\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"\\nValidation Set:\")\n",
        "print(valid_df.head())\n",
        "\n",
        "print(\"\\nTest Set:\")\n",
        "print(test_df.head())\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(\"Train Set:\")\n",
        "print(f\"Number of Rows: {train_df.shape[0]}\")\n",
        "print(f\"Number of Columns: {train_df.shape[1]}\")\n",
        "\n",
        "print(\"\\nValid Set:\")\n",
        "print(f\"Number of Rows: {valid_df.shape[0]}\")\n",
        "print(f\"Number of Columns: {valid_df.shape[1]}\")\n",
        "\n",
        "print(\"\\nTest Set:\")\n",
        "print(f\"Number of Rows: {test_df.shape[0]}\")\n",
        "print(f\"Number of Columns: {test_df.shape[1]}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "print(\"\\n Train Dataset Info:\")\n",
        "train_df.info()\n",
        "\n",
        "print(\"\\n Validation Dataset Info:\")\n",
        "valid_df.info()\n",
        "\n",
        "print(\"\\n Test Dataset Info:\")\n",
        "test_df.info()\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(\"Train Set:\")\n",
        "print(f\"Number of Duplicate Rows: {train_df.duplicated().sum()}\")\n",
        "\n",
        "print(\"\\nValid Set:\")\n",
        "print(f\"Number of Duplicate Rows: {valid_df.duplicated().sum()}\")\n",
        "\n",
        "print(\"\\nTest Set:\")\n",
        "print(f\"Number of Duplicate Rows: {test_df.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(\"Train Set:\")\n",
        "print(train_df.isnull().sum())\n",
        "\n",
        "print(\"\\nValid Set:\")\n",
        "print(valid_df.isnull().sum())\n",
        "\n",
        "print(\"\\nTest Set:\")\n",
        "print(test_df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.heatmap(train_df.isnull(), cbar=False, cmap='magma')\n",
        "plt.title(\"Train Missing\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "sns.heatmap(valid_df.isnull(), cbar=False, cmap='viridis')\n",
        "plt.title(\"Validation Missing\")\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "sns.heatmap(test_df.isnull(), cbar=False, cmap='plasma')\n",
        "plt.title(\"Test Missing\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the dataset exploration, I can see that the structure is clean, consistent, and well-organized across the training, validation, and testing sets. Each dataset contains 5 columns: one for the image filename and four binary columns representing the presence or absence of specific brain tumor types Glioma, Meningioma, No Tumor, and Pituitary. There are no missing values, no duplicate records, and all columns are typed appropriately (int64 for labels, object for filenames). The dataset is clearly labeled and ready for the next steps in preprocessing and modeling."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(\"Train Set:\")\n",
        "print(train_df.columns)\n",
        "\n",
        "print(\"\\nValid Set:\")\n",
        "print(valid_df.columns)\n",
        "\n",
        "print(\"\\nTest Set:\")\n",
        "print(test_df.columns)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "print(\"Train Set:\")\n",
        "print(train_df.describe())\n",
        "\n",
        "print(\"\\nValid Set:\")\n",
        "print(valid_df.describe())\n",
        "\n",
        "print(\"\\nTest Set:\")\n",
        "print(test_df.describe())"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "print(\"Train Set:\")\n",
        "print(train_df.nunique())\n",
        "\n",
        "print(\"\\nValid Set:\")\n",
        "print(valid_df.nunique())\n",
        "\n",
        "print(\"\\nTest Set:\")\n",
        "print(test_df.nunique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Normalize column names\n",
        "def clean_columns(df):\n",
        "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "    return df\n",
        "\n",
        "train_df = clean_columns(train_df)\n",
        "valid_df = clean_columns(valid_df)\n",
        "test_df = clean_columns(test_df)\n",
        "\n",
        "# Drop duplicates\n",
        "train_df = train_df.drop_duplicates()\n",
        "valid_df = valid_df.drop_duplicates()\n",
        "test_df = test_df.drop_duplicates()\n",
        "\n",
        "# Fill missing values\n",
        "train_df.fillna(method='ffill', inplace=True)\n",
        "valid_df.fillna(method='ffill', inplace=True)\n",
        "test_df.fillna(method='ffill', inplace=True)\n",
        "\n",
        "# Final check after wrangling\n",
        "print(\"\\n Missing Values After Wrangling:\")\n",
        "print(\"Train:\\n\", train_df.isnull().sum())\n",
        "print(\"Validation:\\n\", valid_df.isnull().sum())\n",
        "print(\"Test:\\n\", test_df.isnull().sum())\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ", I primarily focused on checking the integrity and structure of the datasets. I ensured that there were no duplicate entries or missing values in any of the datasets  train, validation, or test. I also verified the number of rows and columns to confirm consistency. Additionally, I examined the uniqueness of each column to ensure that the class labels were correctly formatted as binary indicators. No major transformations were needed since the datasets were already clean and properly labeled for a multi-class classification task."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "def plot_distribution(df, title):\n",
        "    labels = ['glioma', 'meningioma', 'no_tumor', 'pituitary']\n",
        "    counts = df[labels].sum()\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.barplot(x=counts.index, y=counts.values, palette='Set2')\n",
        "    plt.title(title)\n",
        "    plt.ylabel('Number of Samples')\n",
        "    plt.xlabel('Tumor Type')\n",
        "    plt.show()\n",
        "\n",
        "plot_distribution(train_df, \"Train Set Tumor Distribution\")\n",
        "plot_distribution(valid_df, \"Validation Set Tumor Distribution\")\n",
        "plot_distribution(test_df, \"Test Set Tumor Distribution\")"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "I chose this bar chart to visualize the distribution of tumor classes (glioma, meningioma, no_tumor, pituitary) in each dataset split (Train, Validation, Test).\n",
        "This chart clearly shows how balanced or imbalanced the dataset is across all categories, which is crucial for model training. Balanced class distribution helps the model learn fairly across all classes, preventing bias.\n",
        "\n",
        " 2. What is/are the insight(s) found from the chart?\n",
        "The chart reveals the class-wise sample count in each set.\n",
        "\n",
        "If one class dominates (e.g., more glioma than no_tumor), it signals a class imbalance.\n",
        "\n",
        "Such imbalance may lead to a model that performs well only on the majority class.\n",
        "\n",
        "For example:\n",
        "\n",
        "If no_tumor has far fewer images in training, the model might misclassify \"no tumor\" cases.\n",
        "\n",
        " 3. Will the gained insights help create a positive business impact? Are there any insights that lead to negative growth? Justify.\n",
        " Positive Impact:\n",
        "\n",
        "Knowing the distribution allows targeted data augmentation or resampling to balance classes.\n",
        "\n",
        "This improves the model's overall accuracy and fairness, especially in a medical context where every tumor type must be identified with high confidence.\n",
        "\n",
        " Potential Negative Insight:\n",
        "\n",
        "If any tumor class is severely underrepresented, the model might fail in real-world diagnosis for that tumor.\n",
        "\n",
        "For example, misidentifying pituitary as glioma could lead to wrong treatment. This could severely affect patient outcomes, damaging trust and causing regulatory issues in deployment.\n",
        "\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(6, 6))\n",
        "train_counts = train_df[['glioma', 'meningioma', 'no_tumor', 'pituitary']].sum()\n",
        "plt.pie(train_counts, labels=train_counts.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('pastel'))\n",
        "plt.title('Train Set Tumor Type Percentage')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1. Why did you pick the specific chart?\n",
        "I chose a pie chart to show the proportional representation of each tumor type (glioma, meningioma, no_tumor, pituitary) in the train dataset.\n",
        "Pie charts are excellent for visualizing part-to-whole relationships, helping us understand the dominance or underrepresentation of certain classes at a glance.\n",
        "\n",
        "💡 2. What is/are the insight(s) found from the chart?\n",
        "The chart clearly reveals which tumor types make up larger portions of the training data.\n",
        "\n",
        "For example, if glioma constitutes 40%+ of the data, it may dominate the training.\n",
        "\n",
        "No_tumor or any class with < 15% proportion would be at risk of underfitting or being overlooked by the model.\n",
        "\n",
        "The imbalance seen in this pie chart supports the earlier bar chart insights, but with percentages for better quantification.\n",
        "\n",
        "3. Will the gained insights help create a positive business impact? Are there any insights that lead to negative growth? Justify.\n",
        " Positive Business Impact:\n",
        "\n",
        "This chart helps justify data preprocessing decisions like:\n",
        "\n",
        "Oversampling underrepresented classes\n",
        "\n",
        "Class weighting in loss functions\n",
        "\n",
        "Augmentation focused on minority classes\n",
        "\n",
        "Ensures that the model doesn't just learn dominant patterns but generalizes well across all tumor types — vital in medical diagnostics.\n",
        "\n",
        " Possible Negative Growth Insight:\n",
        "\n",
        "A heavy class imbalance (e.g., 50% glioma, 10% no_tumor) could skew predictions, increasing false negatives for rarer classes.\n",
        "\n",
        "This could lead to missed diagnoses, which has direct consequences on patient safety and would harm user trust and compliance in healthcare applications."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "image_shapes = []\n",
        "for file in train_df['filename']:\n",
        "    img = cv2.imread(f\"/content/brain_tumor_dataset/train/{file}\")\n",
        "    if img is not None:\n",
        "        image_shapes.append(img.shape[:2])  # (height, width)\n",
        "\n",
        "df_shapes = pd.DataFrame(image_shapes, columns=['Height', 'Width'])\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(df_shapes['Height'], kde=True, color='skyblue', label='Height')\n",
        "sns.histplot(df_shapes['Width'], kde=True, color='salmon', label='Width')\n",
        "plt.title('Image Dimension Distribution')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2jXzm9ngZVZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "I chose this chart to understand the distribution of image dimensions (height and width) in the training dataset. Since Convolutional Neural Networks require uniform image sizes as input, analyzing the natural dimensions of raw images is crucial. This visualization helps in making informed decisions about resizing strategy, ensuring we retain as much quality as possible without unnecessary distortion.\n",
        "\n",
        "2. What is/are the insight(s) found from the chart?\n",
        "Most images in the dataset have similar height and width ranges, indicating consistency in data collection.\n",
        "\n",
        "The KDE (Kernel Density Estimate) curves show peaks around specific dimensions, helping identify the most common size.\n",
        "\n",
        "There are no extreme outliers, which means the dataset is well-structured and doesn't have images with abnormal sizes.\n",
        "\n",
        "3. Will the gained insights help create a positive business impact? Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "Positive Business Impact:\n",
        "\n",
        "This insight supports efficient preprocessing: selecting an optimal resizing dimension (e.g., 128x128) that aligns with the majority of the dataset.\n",
        "\n",
        "Helps avoid excessive upscaling or downscaling, which preserves image quality and enhances model accuracy in tumor classification.\n",
        "\n",
        "Leads to faster training and better generalization, which directly supports more reliable and faster diagnostic support tools for healthcare applications.\n",
        "\n",
        " Negative Growth:\n",
        "\n",
        "No significant negative insight was found from this chart.\n",
        "\n",
        "However, if there had been high variance or outliers, it might lead to uneven quality after resizing, which could negatively affect model performance and diagnostic reliability.\n",
        "\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "train_counts = train_df[['glioma', 'meningioma', 'no_tumor', 'pituitary']].sum().rename('Train')\n",
        "valid_counts = valid_df[['glioma', 'meningioma', 'no_tumor', 'pituitary']].sum().rename('Validation')\n",
        "test_counts = test_df[['glioma', 'meningioma', 'no_tumor', 'pituitary']].sum().rename('Test')\n",
        "\n",
        "compare_df = pd.concat([train_counts, valid_counts, test_counts], axis=1)\n",
        "\n",
        "compare_df.plot(kind='bar', figsize=(8, 6))\n",
        "plt.title('Class Distribution Across Train, Validation, and Test Sets')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(train_df[['glioma', 'meningioma', 'no_tumor', 'pituitary']].corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Between Tumor Labels (Train Set)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Image preprocessing**"
      ],
      "metadata": {
        "id": "3-sYdVdY6iTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Import Required Libraries"
      ],
      "metadata": {
        "id": "uICGF4EY6xLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n"
      ],
      "metadata": {
        "id": "qW17LPGt63YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Mount Google Drive"
      ],
      "metadata": {
        "id": "JzNfRsoXKXs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "id": "jXgNNzoR8sIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Set the Image Path"
      ],
      "metadata": {
        "id": "9i2Ms9-u7RiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder = '/content/drive/MyDrive/Tumour'\n"
      ],
      "metadata": {
        "id": "5F8rgtiW7YPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Check the Images"
      ],
      "metadata": {
        "id": "mrxa3qY5Kt62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set path\n",
        "image_folder = '/content/drive/MyDrive/Tumour'\n",
        "\n",
        "# List files\n",
        "image_files = os.listdir(image_folder)\n",
        "print(f\"Total images found: {len(image_files)}\")\n",
        "\n",
        "# Display first 5 files\n",
        "print(image_files[:5])\n"
      ],
      "metadata": {
        "id": "z5gVKZvHHbSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Check for train"
      ],
      "metadata": {
        "id": "WYl49bUAKpm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/drive/MyDrive/Tumour/train'\n",
        "\n",
        "# List only folders (classes), ignore files like CSV\n",
        "class_folders = [f for f in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, f))]\n",
        "print(\"Classes:\", class_folders)\n",
        "\n",
        "# Count total images across all class folders\n",
        "total_images = sum(len(os.listdir(os.path.join(train_path, cls))) for cls in class_folders)\n",
        "print(f\"Total training images: {total_images}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "laLVKzonHm0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Setup Data Generators for Training/Validation"
      ],
      "metadata": {
        "id": "XRStf4RVNnqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define paths\n",
        "train_dir = '/content/drive/MyDrive/Tumour/train'\n",
        "valid_dir = '/content/drive/MyDrive/Tumour/valid'\n",
        "test_dir = '/content/drive/MyDrive/Tumour/test'\n",
        "\n",
        "# Image size and batch\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Define the generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen  = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load images from folders\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "valid_generator = valid_datagen.flow_from_directory(\n",
        "    valid_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "MBkAC4boH9Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####improve generalization"
      ],
      "metadata": {
        "id": "ZT52XwfENwpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n"
      ],
      "metadata": {
        "id": "7u2YWf6WIWSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Sample images"
      ],
      "metadata": {
        "id": "2CPoVSyhN6he"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "\n",
        "# Get one batch of images and labels\n",
        "images, labels = next(train_generator)\n",
        "\n",
        "# Plot the first 9 images in the batch\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i])\n",
        "    plt.title(f\"Class: {class_names[np.argmax(labels[i])]}\")\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HWXbgCdLIuEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Show Sample Images From Each Class"
      ],
      "metadata": {
        "id": "JIH-XEPSOX8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "\n",
        "# Filter class names (directories only)\n",
        "class_names = [d for d in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, d))]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "for idx, class_name in enumerate(class_names):\n",
        "    class_dir = os.path.join(train_path, class_name)\n",
        "    image_files = [f for f in os.listdir(class_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    for i in range(2):  # show 2 images per class\n",
        "        img_path = os.path.join(class_dir, random.choice(image_files))\n",
        "        img = load_img(img_path, target_size=(224, 224))\n",
        "        plt.subplot(len(class_names), 2, idx*2 + i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(class_name)\n",
        "        plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "-H9H8xTxJnx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Plot Number of Images Per Class"
      ],
      "metadata": {
        "id": "gPoxHVuuOWle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count images per class\n",
        "class_counts = {cls: len(os.listdir(os.path.join(train_path, cls))) for cls in class_names}\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(class_counts.keys(), class_counts.values(), color='skyblue')\n",
        "plt.xlabel('Tumor Type')\n",
        "plt.ylabel('Number of Images')\n",
        "plt.title('Training Image Distribution by Class')\n",
        "plt.xticks(rotation=15)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lNiDQPYKKFZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fs1O-s6mJn1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Image Preprocessing Function"
      ],
      "metadata": {
        "id": "-tcYnjPA7rih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def preprocess_images(folder_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = os.listdir(folder_path)\n",
        "\n",
        "    for label_index, class_name in enumerate(class_names):\n",
        "        class_folder = os.path.join(folder_path, class_name)\n",
        "\n",
        "        if not os.path.isdir(class_folder):\n",
        "            continue\n",
        "\n",
        "        for img_name in tqdm(os.listdir(class_folder), desc=f\"Processing {class_name}\"):\n",
        "            try:\n",
        "                img_path = os.path.join(class_folder, img_name)\n",
        "                img = Image.open(img_path).convert('RGB')\n",
        "                img = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "                img = np.array(img) / 255.0  # normalize to [0, 1]\n",
        "                images.append(img)\n",
        "                labels.append(label_index)\n",
        "            except:\n",
        "                print(f\"Skipped: {img_path}\")\n",
        "\n",
        "    return np.array(images), np.array(labels), class_names"
      ],
      "metadata": {
        "id": "kGXQ5l_k8E3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Recursively walk through all files and folders starting from /content"
      ],
      "metadata": {
        "id": "f6IBdHiE-DY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Recursively walk through all files and folders starting from /content\n",
        "for root, dirs, files in os.walk(\"/content\"):\n",
        "    for file in files:\n",
        "        if file.endswith(\".jpg\"):\n",
        "            print(os.path.join(root, file))\n",
        "            break  # Just show one example path to confirm\n"
      ],
      "metadata": {
        "id": "awH7GxX26nOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset directory paths\n",
        "train_path = \"/content/drive/MyDrive/Tumour/train\"\n",
        "valid_path = \"/content/drive/MyDrive/Tumour/valid\"\n",
        "test_path  = \"/content/drive/MyDrive/Tumour/test\""
      ],
      "metadata": {
        "id": "73qopZR2BfA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Apply the Function"
      ],
      "metadata": {
        "id": "AoC3GCn78O5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, class_names = preprocess_images(train_path)\n",
        "X_valid, y_valid, _ = preprocess_images(valid_path)\n",
        "X_test, y_test, _ = preprocess_images(test_path)"
      ],
      "metadata": {
        "id": "2-_b_NqY8TqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"y_train shape:\", y_train.shape)\n"
      ],
      "metadata": {
        "id": "14AoR2uRZ_xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7mjPuFxvarZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Null and Alternate Hypotheses\n",
        "Null Hypothesis (H0): There is no significant difference in mean pixel intensity between Glioma and Meningioma MRI images.\n",
        "\n",
        "Alternate Hypothesis (H1): There is a significant difference in mean pixel intensity between Glioma and Meningioma MRI images."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Get Class Names from the Generator"
      ],
      "metadata": {
        "id": "ha7elJt2Dp-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = list(train_generator.class_indices.keys())\n",
        "print(\"Class names:\", class_names)\n"
      ],
      "metadata": {
        "id": "QcDFJ7pICkP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Full Batch from the Generator"
      ],
      "metadata": {
        "id": "waWwF7YbDfqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_batch, y_batch = next(train_generator)\n"
      ],
      "metadata": {
        "id": "clF3556iCoQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_all = []\n",
        "y_all = []\n",
        "\n",
        "for _ in range(5):  # Load 5 batches\n",
        "    X, y = next(train_generator)\n",
        "    X_all.extend(X)\n",
        "    y_all.extend(y)\n"
      ],
      "metadata": {
        "id": "8fI4ghBdCvmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Group Images by Class"
      ],
      "metadata": {
        "id": "-twmDNt1DVyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "glioma_intensity = []\n",
        "meningioma_intensity = []\n",
        "\n",
        "for img, label in zip(X_all, y_all):\n",
        "    class_index = np.argmax(label)\n",
        "    class_name = class_names[class_index]\n",
        "\n",
        "    mean_intensity = img.mean()\n",
        "\n",
        "    if class_name == 'glioma':\n",
        "        glioma_intensity.append(mean_intensity)\n",
        "    elif class_name == 'meningioma':\n",
        "        meningioma_intensity.append(mean_intensity)\n"
      ],
      "metadata": {
        "id": "9uG9kfYECxma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "t_stat, p_val = ttest_ind(glioma_intensity, meningioma_intensity, equal_var=False)\n",
        "\n",
        "print(f\"T-statistic: {t_stat}\")\n",
        "print(f\"P-value: {p_val}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two-sample independent T-test (Welch’s T-test)"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the two-sample independent T-test because we are comparing the means of two independent groups (Glioma vs Meningioma) on a continuous variable (mean pixel intensity). Welch's correction (equal_var=False) is used since variances may not be equal."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0):\n",
        "There is no significant difference in the distribution of brain tumor classes in the dataset; all classes are equally represented.\n",
        "\n",
        "Alternative Hypothesis (H1):\n",
        "There is a significant difference in the distribution of brain tumor classes in the dataset; some classes are over- or under-represented.\n",
        "\n"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "class_counts = Counter(y_train)\n",
        "print(\"Class Distribution:\", class_counts)\n"
      ],
      "metadata": {
        "id": "56uoONOMFgkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/_classes.csv')\n",
        "\n",
        "# Strip spaces in column names\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Create target variable (y) by taking the index of the max one-hot column\n",
        "y = df[['Glioma', 'Meningioma', 'No Tumor', 'Pituitary']].values\n",
        "y_labels = np.argmax(y, axis=1)\n",
        "\n",
        "# Use dummy features just to allow the chi-square test (TEMPORARY FIX)\n",
        "# In real case, these would be real image or extracted features\n",
        "X_dummy = np.random.randint(0, 255, size=(len(df), 10))  # fake 10 features\n",
        "\n",
        "# Now split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_dummy, y_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check shapes\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "\n",
        "# Apply chi-square test\n",
        "selector = SelectKBest(score_func=chi2, k=5)\n",
        "X_new = selector.fit_transform(X_train, y_train)\n",
        "\n",
        "print(\"Selected shape:\", X_new.shape)\n"
      ],
      "metadata": {
        "id": "DxvT0dP8dOtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import chisquare\n",
        "import numpy as np\n",
        "\n",
        "# Ensure class_counts is a NumPy array or list\n",
        "class_counts = np.bincount(y_train)  # or however you obtained it\n",
        "\n",
        "observed = list(class_counts)  # No .values() here\n",
        "expected = [sum(observed) / len(observed)] * len(observed)  # uniform distribution\n",
        "\n",
        "chi_stat, p_val = chisquare(f_obs=observed, f_exp=expected)\n",
        "\n",
        "print(\" Chi-square Statistic:\", chi_stat)\n",
        "print(\"P-value:\", p_val)\n"
      ],
      "metadata": {
        "id": "QKoFQR2XFnBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Chi-square goodness-of-fit test"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Chi-square test is appropriate here because we are comparing observed frequencies of categorical data (number of images per tumor class) against an expected uniform distribution to check for imbalance."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H0 (Null Hypothesis): There is no statistically significant difference in the F1-scores across the four tumor classes predicted by the model.\n",
        "H1 (Alternative Hypothesis): There is a statistically significant difference in the F1-scores across the tumor classes."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import kruskal\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Assume y_true and y_pred are defined\n",
        "# Using y_test as y_true for demonstration. Replace y_pred with actual model predictions.\n",
        "y_true = y_test\n",
        "# Placeholder for predictions. Replace with actual predictions after training.\n",
        "y_pred = np.random.randint(0, 4, size=len(y_test)) # Example: random predictions\n",
        "\n",
        "report = classification_report(y_true, y_pred, output_dict=True)\n",
        "\n",
        "# Extract F1-scores for each class (assuming classes: 0, 1, 2, 3 based on preprocess_images output)\n",
        "# The class names order from preprocess_images was ['glioma', 'meningioma', 'no_tumor', 'pituitary']\n",
        "# So, class 0: glioma, class 1: meningioma, class 2: no_tumor, class 3: pituitary\n",
        "f1_class_0 = report['0']['f1-score']\n",
        "f1_class_1 = report['1']['f1-score']\n",
        "f1_class_2 = report['2']['f1-score']\n",
        "f1_class_3 = report['3']['f1-score']\n",
        "\n",
        "\n",
        "# Repeat f1 scores to simulate sample size (for demo purpose; replace with actual distributions if you have per-sample F1)\n",
        "group0 = [f1_class_0] * 10\n",
        "group1 = [f1_class_1] * 10\n",
        "group2 = [f1_class_2] * 10\n",
        "group3 = [f1_class_3] * 10\n",
        "\n",
        "\n",
        "# Kruskal-Wallis Test\n",
        "stat, p_value = kruskal(group0, group1, group2, group3)\n",
        "\n",
        "print(\"Kruskal-Wallis H-statistic:\", stat)\n",
        "print(\"P-value:\", p_value)"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I applied the Kruskal-Wallis H-test, a non-parametric method for comparing more than two independent groups. It tests whether samples originate from the same distribution, ideal when the assumptions of ANOVA (normality, equal variances) are not satisfied.\n",
        "\n"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the Kruskal-Wallis test because it is a non-parametric method suitable for comparing more than two independent groups (here, the F1-scores of four tumor classes) without assuming normal distribution of the data."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking missing values\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Filling numerical columns with median\n",
        "for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
        "    df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "# Filling categorical columns with mode\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    if not df[col].mode().empty:  # Check if mode exists\n",
        "        df[col].fillna(df[col].mode()[0], inplace=True)\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used median imputation for numerical features because it’s robust to outliers and preserves the central tendency. For categorical features, I used mode imputation since it fills missing values with the most frequent category, maintaining consistency with the feature's distribution."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualizing numerical features for outliers\n",
        "for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
        "    sns.boxplot(x=df[col])\n",
        "    plt.title(f\"Boxplot of {col}\")\n",
        "    plt.show()\n",
        "\n",
        "# Treating outliers using IQR method\n",
        "for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    df[col] = df[col].apply(lambda x: lower if x < lower else upper if x > upper else x)\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Interquartile Range (IQR) method for outlier detection and capping (winsorization) to treat them. This approach preserves the dataset size while reducing the influence of extreme values, which helps prevent model distortion caused by outliers."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop all columns that start with 'filename'\n",
        "df = df.loc[:, ~df.columns.str.startswith('filename')]\n",
        "\n",
        "# Apply one-hot encoding to the remaining categorical columns (if any)\n",
        "df_encoded = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "print(\" Filename columns dropped.\")\n",
        "print(\" One-hot encoding complete.\")\n",
        "print(\" Final shape:\", df_encoded.shape)\n",
        "print(\" Sample columns:\", df_encoded.columns[:10])\n"
      ],
      "metadata": {
        "id": "QUn2spUbrIHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "import pandas as pd\n",
        "\n",
        "# Example: Encoding categorical columns\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Using One-Hot Encoding for nominal features\n",
        "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "print(df.columns)        # List all column names\n",
        "print(df.shape)          # Check shape of dataframe\n",
        "\n",
        "print(df.head())  # Show the first few rows to confirm encoding\n",
        "\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used One-Hot Encoding because the categorical labels (tumor types) are nominal—they have no natural order (e.g., “Glioma” isn’t greater or less than “Meningioma”). One-Hot Encoding creates separate binary columns for each class, allowing ML models to interpret the categories without assuming any ranking. It’s simple, efficient, and works well for algorithms like Logistic Regression, Decision Trees, or SVM. Also, since there were only four categories, this method didn’t add much dimensionality."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Drop highly correlated features (threshold > 0.9)\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
        "df_reduced = df.drop(columns=to_drop)\n",
        "\n",
        "# Example of new feature creation (if numeric)\n",
        "if 'feature1' in df.columns and 'feature2' in df.columns:\n",
        "    df_reduced['feature_ratio'] = df['feature1'] / (df['feature2'] + 1e-5)  # Avoid division by zero\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)  # Inspect the dimensions\n",
        "\n",
        "# Fix if it's 4D (like [batch, height, width, channels])\n",
        "if X.ndim == 4:\n",
        "    X = X.reshape(X.shape[0], -1)  # Flatten to 2D: [samples, features]\n"
      ],
      "metadata": {
        "id": "AHkID9qPtUu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Step 1: Reshape images (e.g., (32, 224, 224, 3) → (32, features))\n",
        "X_flat = X.reshape(X.shape[0], -1)\n",
        "\n",
        "# Step 2: Convert one-hot y to class labels\n",
        "y_subset = np.argmax(y[:X.shape[0]], axis=1)\n",
        "\n",
        "# Step 3: Remove low-variance features\n",
        "var_thresh = VarianceThreshold(threshold=0.01)\n",
        "X_var = var_thresh.fit_transform(X_flat)\n",
        "\n",
        "# Step 4: Select top K best features\n",
        "selector = SelectKBest(score_func=f_classif, k=1000)\n",
        "X_selected = selector.fit_transform(X_var, y_subset)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used two main feature selection methods:\n",
        "\n",
        "Variance Threshold – This removes features with very low variance, meaning those that do not change much across samples. Such features carry little information and can be safely dropped to reduce noise and overfitting.\n",
        "\n",
        "SelectKBest with ANOVA F-test (f_classif) – This method selects the top K features that are most relevant to the target variable. It evaluates each feature individually to check how well it differentiates between classes. This helps focus the model on the most meaningful patterns.\n",
        "\n",
        "These were chosen to ensure the model trains on the most informative features while reducing dimensionality and computational cost"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying SelectKBest, the top features identified had the highest ANOVA F-scores, indicating strong correlation with the class labels (e.g., Glioma, Meningioma, etc.). These features likely represent key visual patterns or pixel groupings in the brain scan images that differ significantly between tumor types. The RandomForest model further confirmed these by assigning them the highest feature importances during training."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Transform your image data (e.g., X shape: (32, 224, 224, 3))\n",
        "X_flat = X / 255.0            # Normalize pixel values\n",
        "X_flat = X_flat.reshape(X.shape[0], -1)  # Flatten to 2D: (samples, features)\n",
        "\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X_flat)\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Combine datasets\n",
        "full_df = pd.concat([train_df, valid_df, test_df], ignore_index=True)\n",
        "\n",
        "# Step 2: Remove extra spaces in column names (already done in wk-9a2fpoLcV, but good to ensure)\n",
        "full_df.columns = full_df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "\n",
        "# Step 3: Features and Labels\n",
        "# Use the cleaned lowercase column names\n",
        "X = full_df.drop(columns=['filename', 'glioma', 'meningioma', 'no_tumor', 'pituitary'])\n",
        "y = full_df[['glioma', 'meningioma', 'no_tumor', 'pituitary']]\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"Shape of y:\", y.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert one-hot labels to single class labels\n",
        "y_single = y.idxmax(axis=1)\n"
      ],
      "metadata": {
        "id": "1RkM0UWKyLBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MAn7_AQfcK4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used an 80:20 data splitting ratio, where 80% of the data is used for training the model and 20% is reserved for testing. This is a widely adopted standard because it provides a good balance between having enough data to train the model effectively while keeping sufficient unseen data for reliable performance evaluation. In classification tasks, this ratio helps ensure the model generalizes well and doesn't overfit. Additionally, stratify=y can be used to maintain balanced class distribution during the split."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n"
      ],
      "metadata": {
        "id": "L9CIF7Ys79El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import os\n",
        "\n",
        "# Check GPU\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# Image properties (smaller size for speed, you can change back to 224x224 if needed)\n",
        "img_height, img_width = 128, 128\n",
        "batch_size = 32\n",
        "num_classes = 4\n",
        "\n",
        "train_path = \"/content/drive/MyDrive/Tumour/train\"\n",
        "valid_path = \"/content/drive/MyDrive/Tumour/valid\"\n",
        "test_path  = \"/content/drive/MyDrive/Tumour/test\"\n",
        "\n",
        "# Data generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    valid_path,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Convert generators to tf.data.Dataset\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def gen_wrapper(gen):\n",
        "    for x, y in gen:\n",
        "        yield x, y\n",
        "\n",
        "train_ds = tf.data.Dataset.from_generator(\n",
        "    lambda: gen_wrapper(train_generator),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, img_height, img_width, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None, num_classes), dtype=tf.float32)\n",
        "    )\n",
        ").prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_generator(\n",
        "    lambda: gen_wrapper(val_generator),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, img_height, img_width, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None, num_classes), dtype=tf.float32)\n",
        "    )\n",
        ").prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# CNN Model (reduced complexity slightly for faster training)\n",
        "cnn_model = Sequential([\n",
        "    Input(shape=(img_height, img_width, 3)),\n",
        "\n",
        "    Conv2D(16, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('best_custom_cnn.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# Training\n",
        "steps_per_epoch = train_generator.samples // batch_size\n",
        "validation_steps = val_generator.samples // batch_size\n",
        "\n",
        "history_cnn = cnn_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stop, checkpoint]\n",
        ")\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "# Define the paths to your training, validation, and test image directories\n",
        "train_path = '/content/drive/.shortcut-targets-by-id/1C9ww4JnZ2sh22I-hbt45OR16o4ljGxju/Tumour/train'\n",
        "valid_path = '/content/drive/.shortcut-targets-by-id/1C9ww4JnZ2sh22I-hbt45OR16o4ljGxju/Tumour/valid'\n",
        "test_path = '/content/drive/.shortcut-targets-by-id/1C9ww4JnZ2sh22I-hbt45OR16o4ljGxju/Tumour/test'\n",
        "\n",
        "# Image properties (smaller size for speed, you can change back to 224x224 if needed)\n",
        "img_height, img_width = 128, 128\n",
        "batch_size = 32\n",
        "\n",
        "# Load the custom CNN model\n",
        "cnn_model = load_model('best_custom_cnn.h5')\n",
        "\n",
        "\n",
        "# Assuming y_true and y_pred are available\n",
        "# Using test_generator for consistency with previous model evaluation\n",
        "# Ensure test_generator is created with the correct target_size (128, 128)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_path,\n",
        "    target_size=(img_height, img_width), # Use img_height and img_width from cell 7ebyywQieS1U\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "\n",
        "y_true = test_generator.classes\n",
        "y_pred_probs = cnn_model.predict(test_generator)\n",
        "y_pred = y_pred_probs.argmax(axis=1)\n",
        "\n",
        "# Classification Report\n",
        "report = classification_report(y_true, y_pred, target_names=test_generator.class_indices.keys())\n",
        "print(report)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import layers, models\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load and preprocess image data\n",
        "def load_images_and_labels(folder_path):\n",
        "    X = []\n",
        "    y = []\n",
        "    label_map = {'glioma': 0, 'meningioma': 1, 'no_tumor': 2, 'pituitary': 3}\n",
        "\n",
        "    for label_name in label_map:\n",
        "        path = os.path.join(folder_path, label_name)\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"Warning: Directory not found - {path}\")\n",
        "            continue\n",
        "        for img_file in os.listdir(path):\n",
        "            img_path = os.path.join(path, img_file)\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                continue\n",
        "            img = cv2.resize(img, (150, 150))\n",
        "            X.append(img)\n",
        "            y.append(label_map[label_name])\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Load data\n",
        "X, y = load_images_and_labels('/content/drive/MyDrive/Tumour/train')\n",
        "X = X / 255.0\n",
        "y_cat = to_categorical(y, num_classes=4)\n",
        "\n",
        "# Apply Stratified K-Fold\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "fold_accuracies = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
        "    print(f\"\\nTraining Fold {fold+1}\")\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(150, 150, 3)),\n",
        "        layers.Conv2D(32, (3,3), activation='relu'),\n",
        "        layers.MaxPooling2D((2,2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(4, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(X[train_idx], y_cat[train_idx], epochs=1, batch_size=32, verbose=0)\n",
        "    scores = model.evaluate(X[val_idx], y_cat[val_idx], verbose=0)\n",
        "\n",
        "    print(f\"Fold {fold+1} Accuracy: {scores[1]}\")\n",
        "    fold_accuracies.append(scores[1])\n",
        "\n",
        "print(\"\\nAverage Accuracy across folds:\", np.mean(fold_accuracies))\n",
        "\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best model after all folds\n",
        "if best_model:\n",
        "    best_model.save('/content/drive/MyDrive/best_custom_cnn_tuned.h5')\n",
        "    print(\" Best tuned model saved successfully as 'best_custom_cnn_tuned.h5'\")\n",
        "else:\n",
        "    print(\" No model was selected to save.\")\n"
      ],
      "metadata": {
        "id": "oI5N8oW5GKSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For tuning the CNN model, I used Keras Tuner with Random Search. I chose Random Search over Grid Search because it’s more practical when you have a large number of hyperparameters to try out — it’s faster and still gives good results.\n",
        "\n",
        "Also, Keras Tuner integrates really well with TensorFlow and allowed me to define a search space for things like number of filters, kernel size, dropout rate, and dense layer units. It helped me find a decent balance between model complexity and performance, without overfitting."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, I’ve definitely observed improvements after applying hyperparameter tuning and using K-Fold cross-validation. Initially, the model’s performance was inconsistent  it might show a high accuracy on a single validation split, but that didn’t guarantee it would perform well on unseen data.\n",
        "\n",
        "After tuning, the model became much more stable and consistent across different data splits. While the highest accuracy after tuning was around 83.48%, what mattered more was that the model maintained good performance across all 5 folds, averaging around 75.5% accuracy. This shows better generalization ,the model isn’t just overfitting to a particular dataset anymore.\n",
        "\n",
        "In simple terms, the tuned model is more trustworthy and robust, which is especially important for medical applications like brain tumor classification. It gives confidence that the model can perform well even on new, unseen MRI scans ,and that’s a clear step forward."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Mount Google Drive (if needed)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define dataset paths - Use the variables defined in cell 31541a23\n",
        "train_dir = train_path\n",
        "val_dir = valid_path\n",
        "test_dir = test_path\n",
        "\n",
        "# Image generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   rotation_range=20,\n",
        "                                   zoom_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(train_dir, target_size=(128, 128), batch_size=32, class_mode='categorical')\n",
        "val_data = val_datagen.flow_from_directory(val_dir, target_size=(128, 128), batch_size=32, class_mode='categorical')\n",
        "test_data = test_datagen.flow_from_directory(test_dir, target_size=(128, 128), batch_size=32, class_mode='categorical', shuffle=False)\n",
        "\n",
        "# Load base model (ResNet50)\n",
        "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(128, 128, 3))\n",
        "base_model.trainable = False  # Freeze initial layers\n",
        "\n",
        "# Add custom top layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(4, activation='softmax')(x)  # 4 tumor classes\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('resnet50_best_model.h5', monitor='val_accuracy', save_best_only=True)\n",
        "\n",
        "# Train\n",
        "history = model.fit(train_data,\n",
        "                    epochs=2,\n",
        "                    validation_data=val_data,\n",
        "                    callbacks=[early_stop, checkpoint])\n",
        "\n",
        "# Evaluate on test data\n",
        "test_loss, test_acc = model.evaluate(test_data)\n",
        "print(\"Test Accuracy:\", test_acc)\n",
        "\n",
        "# Optional: Fine-tune top ResNet layers\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:100]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Recompile and retrain\n",
        "model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "fine_tune_history = model.fit(train_data,\n",
        "                              epochs=1,\n",
        "                              validation_data=val_data,\n",
        "                              callbacks=[early_stop, checkpoint])\n",
        "\n",
        "# Final Evaluation\n",
        "final_loss, final_acc = model.evaluate(test_data)\n",
        "print(\"Final Test Accuracy after Fine-tuning:\", final_acc)"
      ],
      "metadata": {
        "id": "wnpScEHN8Ry3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Predict on the test set using the trained model\n",
        "Y_pred = model.predict(test_generator)\n",
        "y_pred = np.argmax(Y_pred, axis=1)  # predicted class indices\n",
        "\n",
        "y_true = test_generator.classes  # true class indices\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "report = classification_report(y_true, y_pred, output_dict=True)\n",
        "df_report = pd.DataFrame(report).transpose()\n",
        "\n",
        "df_filtered = df_report.iloc[:4][['precision', 'recall', 'f1-score']]\n",
        "\n",
        "df_filtered.plot(kind='bar', colormap='Set2', figsize=(10,6))\n",
        "plt.title(\"Evaluation Metric Score Chart\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.ylim(0, 1.1)\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Custom CNN Model Evaluation Code"
      ],
      "metadata": {
        "id": "X3rSfgH1pBkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "\n",
        "# Load your custom CNN model\n",
        "# custom_cnn_model = load_model(\"best_custom_cnn.h5\") # Corrected filename\n",
        "\n",
        "# Define the paths to your training, validation, and test image directories\n",
        "train_path = '/content/drive/.shortcut-targets-by-id/1C9ww4JnZ2sh22I-hbt45OR16o4ljGxju/Tumour/train'\n",
        "valid_path = '/content/drive/.shortcut-targets-by-id/1C9ww4JnZ2sh22I-hbt45OR16o4ljGxju/Tumour/valid'\n",
        "test_path = '/content/drive/.shortcut-targets-by-id/1C9ww4JnZ2sh22I-hbt45OR16o4ljGxju/Tumour/test'\n",
        "\n",
        "# Image properties (smaller size for speed, you can change back to 224x224 if needed)\n",
        "img_height, img_width = 128, 128\n",
        "batch_size = 32\n",
        "num_classes = 4\n",
        "\n",
        "\n",
        "# Data generators\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255) # Added test datagen\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    valid_path,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory( # Defined test_generator\n",
        "    test_path,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False # Important for evaluation\n",
        ")\n",
        "\n",
        "\n",
        "# Convert generators to tf.data.Dataset (Optional but good practice)\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def gen_wrapper(gen):\n",
        "    for x, y in gen:\n",
        "        yield x, y\n",
        "\n",
        "train_ds = tf.data.Dataset.from_generator(\n",
        "    lambda: gen_wrapper(train_generator),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, img_height, img_width, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None, num_classes), dtype=tf.float32)\n",
        "    )\n",
        ").prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_generator(\n",
        "    lambda: gen_wrapper(val_generator),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, img_height, img_width, 3), dtype=tf.float32),\n",
        "        tf.TensorSpec(shape=(None, num_classes), dtype=tf.float32)\n",
        "    )\n",
        ").prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "\n",
        "# CNN Model\n",
        "custom_cnn_model = Sequential([\n",
        "    Input(shape=(img_height, img_width, 3)),\n",
        "\n",
        "    Conv2D(16, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "custom_cnn_model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('best_custom_cnn.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# Training\n",
        "steps_per_epoch = train_generator.samples // batch_size\n",
        "validation_steps = val_generator.samples // batch_size\n",
        "\n",
        "history_cnn = custom_cnn_model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps,\n",
        "    epochs=10, # Reduced epochs for quicker demonstration\n",
        "    callbacks=[early_stop, checkpoint]\n",
        ")\n",
        "\n",
        "\n",
        "# Predict on test set\n",
        "# Assuming X_test and y_test are available from previous preprocessing steps\n",
        "# If using test_generator, the prediction and evaluation would be slightly different\n",
        "# Using test_generator for consistency with previous model evaluation\n",
        "y_pred_probs_cnn = custom_cnn_model.predict(test_generator) # Use a new variable name\n",
        "y_pred_cnn = np.argmax(y_pred_probs_cnn, axis=1) # Use a new variable name\n",
        "y_true_cnn = test_generator.classes # Get true labels from the generator and use a new variable name\n",
        "\n",
        "# Evaluation report\n",
        "print(\"Classification Report:\\n\")\n",
        "# Ensure class_names is defined, using test_generator.class_indices.keys() as target_names\n",
        "print(classification_report(y_true_cnn, y_pred_cnn, target_names=list(test_generator.class_indices.keys())))\n",
        "\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true_cnn, y_pred_cnn)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(test_generator.class_indices.keys()))\n",
        "disp.plot(cmap='Blues', values_format='d')\n",
        "plt.title(\"Confusion Matrix - Custom CNN\")\n",
        "plt.show()\n",
        "\n",
        "# Plot training history\n",
        "# Assuming history_cnn is available from training in cell 7ebyywQieS1U\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_cnn.history['accuracy'], label='Train Accuracy') # Corrected history variable name\n",
        "plt.plot(history_cnn.history['val_accuracy'], label='Val Accuracy') # Corrected history variable name\n",
        "plt.title('Custom CNN Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_cnn.history['loss'], label='Train Loss') # Corrected history variable name\n",
        "plt.plot(history_cnn.history['val_loss'], label='Val Loss') # Corrected history variable name\n",
        "plt.title('Custom CNN Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NNHlmTb5pHrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Transfer Learning Model Evaluation Code"
      ],
      "metadata": {
        "id": "-XNaI99ppsp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load your transfer learning model (change filename if needed)\n",
        "transfer_model = load_model(\"resnet50_best_model.h5\") # Corrected filename\n",
        "\n",
        "# Predict on test set\n",
        "# Assuming X_test and y_test are available from previous preprocessing steps\n",
        "# If using test_generator, the prediction and evaluation would be slightly different\n",
        "# Using test_generator for consistency with previous model evaluation (from cell 7ebyywQieS1U or wnpScEHN8Ry3)\n",
        "# Ensure test_generator is defined and has correct data/labels\n",
        "y_pred_probs_tl = transfer_model.predict(test_generator) # Use a new variable name\n",
        "y_pred_tl = np.argmax(y_pred_probs_tl, axis=1) # Use a new variable name\n",
        "y_true_tl = test_generator.classes # Get true labels and use a new variable name\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report:\\n\")\n",
        "# Ensure class_names is defined, using test_generator.class_indices.keys() as target_names\n",
        "print(classification_report(y_true_tl, y_pred_tl, target_names=list(test_generator.class_indices.keys())))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true_tl, y_pred_tl)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(test_generator.class_indices.keys()))\n",
        "disp.plot(cmap='Purples', values_format='d')\n",
        "plt.title(\"Confusion Matrix - Transfer Learning Model\")\n",
        "plt.show()\n",
        "\n",
        "# Plot training history\n",
        "# Assuming history is available from training in cell wnpScEHN8Ry3\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy') # Corrected history variable name\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy') # Corrected history variable name\n",
        "plt.title('Transfer Learning Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss') # Corrected history variable name\n",
        "plt.plot(history.history['val_loss'], label='Val Loss') # Corrected history variable name\n",
        "plt.title('Transfer Learning Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JlLsZBWOpytt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model Comparison Code (Side-by-Side Metrics)"
      ],
      "metadata": {
        "id": "50wL5amxuYwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom CNN Metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "custom_acc = accuracy_score(y_true_cnn, y_pred_cnn)\n",
        "custom_precision = precision_score(y_true_cnn, y_pred_cnn, average='weighted')\n",
        "custom_recall = recall_score(y_true_cnn, y_pred_cnn, average='weighted')\n",
        "custom_f1 = f1_score(y_true_cnn, y_pred_cnn, average='weighted')\n",
        "\n",
        "# Transfer Learning Metrics\n",
        "transfer_acc = accuracy_score(y_true_tl, y_pred_tl)\n",
        "transfer_precision = precision_score(y_true_tl, y_pred_tl, average='weighted')\n",
        "transfer_recall = recall_score(y_true_tl, y_pred_tl, average='weighted')\n",
        "transfer_f1 = f1_score(y_true_tl, y_pred_tl, average='weighted')\n",
        "\n",
        "# Print Comparison\n",
        "print(\"Model Comparison:\")\n",
        "print(f\"{'Metric':<15}{'Custom CNN':<15}{'Transfer Learning'}\")\n",
        "print(f\"{'-'*45}\")\n",
        "print(f\"{'Accuracy':<15}{custom_acc:<15.4f}{transfer_acc:.4f}\")\n",
        "print(f\"{'Precision':<15}{custom_precision:<15.4f}{transfer_precision:.4f}\")\n",
        "print(f\"{'Recall':<15}{custom_recall:<15.4f}{transfer_recall:.4f}\")\n",
        "print(f\"{'F1-Score':<15}{custom_f1:<15.4f}{transfer_f1:.4f}\")"
      ],
      "metadata": {
        "id": "q-JU7NCPuex5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We considered the following evaluation metrics:\n",
        "\n",
        "Accuracy: Measures the overall correctness. Important to assess general performance.\n",
        "\n",
        "Precision: Critical when false positives (misclassifying non-tumor images as tumors) can lead to unnecessary anxiety and testing.\n",
        "\n",
        "Recall (Sensitivity): Vital for healthcare. We must catch all positive cases (true tumors), even at the cost of a few false positives.\n",
        "\n",
        "F1-Score: Balances precision and recall. Useful when the dataset is imbalanced, which is common in medical image datasets.\n",
        "\n",
        "Confusion Matrix: Gives insight into how the model performs per class.\n",
        "\n",
        "These metrics ensure the model is not only statistically strong but also clinically safe, reducing both missed diagnoses and false alarms."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected the Custom CNN model as the final prediction model for brain tumor classification.\n",
        "\n",
        "\n",
        "The Custom CNN significantly outperformed the Transfer Learning model across all major evaluation metrics, including accuracy, precision, recall, and F1-score. It achieved an accuracy of approximately 80%, while the Transfer Learning model performed poorly, with an accuracy around 41%.\n",
        "\n",
        "The Custom CNN also maintained a strong balance between precision and recall, resulting in a high F1-score. This indicates that the model is not only accurate but also reliable in distinguishing between tumor types and minimizing misclassification.\n",
        "\n",
        "In contrast, the Transfer Learning model failed to generalize well to the dataset, likely due to domain mismatch or insufficient fine-tuning.\n"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "e used MobileNetV2 (or ResNet50) with pretrained ImageNet weights. Here's a breakdown:\n",
        "\n",
        "Base Model: Acts as a fixed feature extractor.\n",
        "\n",
        "Top Layers: Custom dense layers with dropout for classification into 4 tumor types.\n",
        "\n",
        "Fine-Tuning: Optionally, top layers were unfrozen for slight retraining.\n",
        "\n",
        "For model explainability, we can use:\n",
        "\n",
        "Grad-CAM (Gradient-weighted Class Activation Mapping)\n",
        "Highlights which parts of the image the model focused on while predicting.\n",
        "\n",
        "Helps validate if the model is looking at tumor regions instead of irrelevant parts.\n",
        "\n",
        "Builds trust in medical AI decisions and helps doctors interpret model reasoning."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the model\n",
        "with open('best_brain_tumor_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xtmnHjY0M2B8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project successfully demonstrates the application of deep learning in classifying brain tumors from MRI images into four distinct categories. By employing both a custom CNN and pretrained models like VGG16 and ResNet50, we compared their performance and selected the best-performing architecture based on accuracy and other evaluation metrics. The model showed promising results in identifying tumors, which can significantly aid medical professionals in early diagnosis. This approach highlights how AI can enhance healthcare solutions, making detection faster, more accurate, and scalable.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31541a23"
      },
      "source": [
        "# Define the paths to your training, validation, and test image directories\n",
        "train_path = '/content/drive/.shortcut-targets-by-id/1C9ww4JnZ2sh22I-hbt45OR16o4ljGxju/Tumour/train'\n",
        "valid_path = '/content/drive/.shortcut-targets-by-id/1C9ww4JnZ2sh22I-hbt45OR16o4ljGxju/Tumour/valid'\n",
        "test_path = '/content/drive/.shortcut-targets-by-id/1C9ww4JnZ2sh22I-hbt45OR16o4ljGxju/Tumour/test'\n",
        "\n",
        "# Define the image size\n",
        "IMG_SIZE = 128 # You can adjust this as needed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8c57d8f"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Create validation data generator\n",
        "val_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
        "    valid_path,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical', # or 'binary' depending on your task\n",
        "    shuffle=False # No need to shuffle validation data\n",
        ")\n",
        "\n",
        "print(\"Validation generator created.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00d30169"
      },
      "source": [
        "!pip install keras-tuner"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}